# -*- coding: utf-8 -*-
"""Copy of Project_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10JUDVawrSyP9_p-IU1pdj-bNMomPoGWw
"""

import time, math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from statsmodels.stats.outliers_influence import variance_inflation_factor

!pip install scikit-learn
import scipy
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.impute import KNNImputer
from sklearn.preprocessing import PowerTransformer
from scipy import stats as stat_
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import  DecisionTreeClassifier
from sklearn import svm
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, precision_recall_fscore_support, roc_curve, f1_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.tree import  DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#%cd /content/drive/My Drive
# %cd /content/drive/My Drive



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/ML Project/

Data_Reading = pd.read_csv('terrorismdb_0522.csv',encoding = "ISO-8859-1", low_memory=False)

Data_Reading.shape

Data_Reading.info()

Data_Reading_copy = Data_Reading.copy()
Data_Reading_copy1 = Data_Reading.copy()

count = 0
no_null = []
for i in Data_Reading.columns:
    data_col = Data_Reading[i]
    #print (i, " ",round((data_col.isnull().sum()/len(gt)*100), 4), gt[i].dtype)
    if data_col.isnull().sum()/len(Data_Reading)*100 < 10:
        count += 1
        no_null.append(i)

print(f'columns with less than 10% null values: {count}')

Data_Reading_copy_new=Data_Reading_copy.rename(columns={'iyear':'Year','imonth':'Month','iday':'Day','approxdate':'Date','country_txt':'Country','success':'Success_rate','attacktype1_txt':'Attack_type','targtype1_txt':'Target','nkill':'Killed','weaptype1_txt':'Weapon_type','nwound':'Wounded','city':'City','gname':'Attacker'})

Data_Reading_copy_new=Data_Reading_copy_new[['Year','Day','Month','Date','Country','City','Success_rate','Attack_type','Target','Killed','Wounded','Weapon_type','Attacker']]
Data_Reading_copy_new

Data_Reading_copy_new.isnull().sum()

Data_Reading_copy_new.drop(columns="Date", inplace=True)

Data_Reading_copy_new.isnull().sum()

Data_Reading_copy_new.dropna(inplace=True)

Data_Reading_copy_new.isnull().sum()

Data_Reading['casualities'] = (Data_Reading['nkill'] + Data_Reading['nwound'])  #add column for number of casualities = killed + wounded
Data_Reading['nclass'] = (Data_Reading.casualities.apply(lambda x: 0 if x == 0 else 1))  #create binary interpretation for casualities

# Country-wise analysis of Terror Attacks and their destruction

def country_analysis(data , country=None , feature1=None , feature2=None , line_color=None , palette=None , title =None, start_year = None, end_year =None):
    filtered_data = data[(data['iyear'] >= start_year) & (data['iyear'] <= end_year)]
    plt.figure(figsize = (25,8))
    #Plot 1
    plt.subplot(1,2,1)
    custom_palette = sns.color_palette(['#FF5733', '#33FF57', '#334CFF'])
    sns.countplot(x = filtered_data.loc[filtered_data['country_txt'] == country]['city'] ,\
                  order = filtered_data.loc[filtered_data.country_txt == country].city.value_counts()[:5].index,\
                  data = filtered_data ,\
                  palette = "Set3")
    plt.xticks(rotation = 90)
    plt.title("Attacks count in different cities of %s" % (country), weight = 'bold' ,fontsize = 18 ,loc = 'center')
    plt.xlabel('City',fontsize = 20)
    plt.ylabel('Count',fontsize = 20)



    #Plot 3
    plt.subplot(1,2,2)
    filtered_data.query('country_txt == @country').groupby(['iyear'])['casualities'].sum().plot(xticks = filtered_data.loc[filtered_data.country_txt == country].groupby(['iyear'])['casualities'].sum().index, rot=90 , color = line_color , linewidth=5, marker='o', markersize=8)
    plt.title("Count of casualities from (2000 - 2021) in %s" % (country), fontdict = {'fontsize' : 18 , 'weight': 'bold' } , loc = 'center')
    plt.ylabel("Casualities" ,fontsize = 20)
    plt.xlabel('Year',fontsize = 20)

    plt.subplots_adjust(wspace = 1 ,hspace =0.5 ,top = 0.5 , bottom = 0.1)  # tune the subplot layout
    plt.tight_layout()   #to avoid overlapping of subplots and fit them cleanly in figure
    plt.show()

country_analysis(Data_Reading, country= 'Ukraine', feature1= 'nkill' , feature2= 'attacktype1_txt' , line_color='green' , palette=None , title =None, start_year= 2000, end_year= 2021)

custom_palette = sns.color_palette("RdYlGn", n_colors=2)
inverted_palette = custom_palette[::-1]
sns.countplot(x ='success', data = Data_Reading_copy, palette = inverted_palette)
plt.xlabel('Failed(success=0) and Success = 1')
plt.title('Distribution of attacks failure and success attempts',fontsize = 14, weight = 'bold', loc = 'center')

sns.countplot(y ='attacktype1_txt', hue = 'success', data = Data_Reading_copy, palette = 'deep')

plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.title('Different kinds of attack attempts and their distribution of Failure and Success',fontsize = 14, weight = 'bold', loc = 'center')

custom_palette = sns.color_palette("Blues_r", n_colors=1) + sns.color_palette("Reds_r", n_colors=4)
sns.countplot(y ='targtype1_txt', hue = 'success', data = Data_Reading_copy, palette = custom_palette)
plt.title("Target Locations distribution based on their type",fontsize = 14, weight = 'bold', loc = 'center')
fig=plt.gcf()
fig.set_size_inches(10,8)

custom_palette = sns.color_palette("YlOrBr_r", n_colors=1) + sns.color_palette("Purples_r", n_colors=4)
sns.countplot(x ='guncertain1', hue = 'success', data = Data_Reading_copy, palette = custom_palette)
plt.xlabel("Gun not used = 0.0 , Gun Used =1.0")
plt.title("Involvement of Gun Usage in attacks",fontsize = 14, weight = 'bold', loc = 'center')

import pandas as pd

# Sample data (replace this with your actual dataset)
Data_Reading_copy = pd.DataFrame({
    'attacktype1_txt': ['Type1', 'Type2', 'Type1', 'Type2', 'Type1'],
    'success': [1, 0, 1, 1, 0],
    'Gun Used': [1, 0, 1, 0, 1]
})

# Calculate success rate for each group
success_rates = Data_Reading_copy.groupby('Gun Used')['success'].mean()

# Display the success rates
print(success_rates)

sns.countplot(y ='weaptype1_txt', hue = 'success', data = Data_Reading_copy1)
plt.title('Usage of types of weapons in successful and Failed attempts',fontsize = 14, weight = 'bold', loc = 'center')

fig, ax = plt.subplots(nrows=1, ncols=1 , squeeze = False , figsize =(20,10))
total = float(len(Data_Reading_copy1))
region_attacktype=pd.crosstab(Data_Reading_copy1.region_txt, Data_Reading_copy1.attacktype1_txt).plot(stacked=True,width=1,color=sns.color_palette('Set1',9),kind = 'barh' , ax = ax[0][0])
plt.title('Distribution of different kinds of attacks based on the regions we have in the data set', weight = 'bold')
plt.legend(fontsize = 12)

fig=plt.gcf()
fig.set_size_inches(18,6)
sns.set()
sns.countplot(x= 'iyear', data= Data_Reading ,palette='Paired')
plt.xticks(rotation=90)
plt.xlabel('Years')
plt.title('Number Of Terrorist Activities Each Year', weight='bold')
plt.show()

attacks_count = Data_Reading.gname.value_counts().to_frame().drop('Unknown').reset_index()[:20]
attacks_count.columns = ['Attacker', 'Total Attacks']
plt.subplots(figsize=(10,8))
sns.barplot(y=attacks_count['Attacker'], x=attacks_count['Total Attacks'], palette='RdYlGn_r')
plt.title('Number of attempts by Attackers', weight = 'bold')
plt.show()

from seaborn.widgets import color_palette
terror_region  = pd.crosstab(Data_Reading_copy1.iyear , Data_Reading_copy1.region_txt )

terror_region.plot(colormap='Set2', linewidth=3, marker='o', markersize=8)
fig=plt.gcf()
fig.set_size_inches(25,10)
plt.legend(loc='upper left')
plt.xlabel('Years')
plt.title("Terrorist attacks over years in different regions", weight = 'bold' ,fontsize = 18 ,loc = 'center')
plt.show()

Data_Reading = Data_Reading.loc[:, ["iyear", "imonth", "iday", "extended","country_txt","vicinity","multiple","success","suicide","attacktype1_txt","targtype1_txt","target1",
         "crit1", "crit2","crit3","doubtterr","gname" ,"guncertain1","weaptype1_txt","nkill","nwound","property","ishostkid"]]

Data_Reading = Data_Reading.loc[(Data_Reading['crit1'] == 1) & (Data_Reading['crit2'] == 1) & (Data_Reading['crit3'] == 1) & (Data_Reading['doubtterr'] == 0)]

Data_Reading = Data_Reading.rename(columns={'iyear':'year','imonth':'month','iday':'day','country_txt':'country','attacktype1_txt':'attack_type','targtype1_txt':'target_type',
                      'weaptype1_txt':'weapon_type','nkill':'killed','nwound':'wounded','gname':'attacker'})

Data_Reading['casualities'] = (Data_Reading['killed'] + Data_Reading['wounded'])
Data_Reading['nclass'] = (Data_Reading['casualities'].apply(lambda x: 0 if x == 0 else 1))

Data_Reading = Data_Reading[["year", "month", "day", "extended","country","vicinity","multiple","success","suicide","attack_type","target_type","target1",
         "attacker" ,"guncertain1","weapon_type","killed","wounded","casualities","nclass","property","ishostkid"]]

Data_Reading = Data_Reading.apply(lambda x: x.fillna(0) if x.name in ['guncertain1', 'multiple', 'ishostkid'] else x, axis=0)
Data_Reading = Data_Reading.apply(lambda x: x.replace(-9, 0) if x.name in ['vicinity', 'property'] else x, axis=0)

fill_dict = {'target_type': 'Unknown', 'weapon_type': 'Unknown', 'attacker': 'Unknown', 'target1': 'unknown'}
Data_Reading = Data_Reading.fillna(value=fill_dict)
Data_Reading['target1'] = Data_Reading['target1'].replace('unk','unknown')

median_fill_cols = ['killed', 'wounded', 'casualities']
for col in median_fill_cols:
    Data_Reading[col] = np.round(Data_Reading[col].fillna(Data_Reading[col].median())).astype(int)

cols_to_fillna = ['guncertain1', 'multiple', 'ishostkid']
Data_Reading[cols_to_fillna] = Data_Reading[cols_to_fillna].fillna(0)

cols_to_replace = ['vicinity', 'property']
Data_Reading[cols_to_replace] = Data_Reading[cols_to_replace].replace(-9, 0)

from sklearn import preprocessing
lb = preprocessing.LabelEncoder()

cols_to_transform = ['country', 'attack_type', 'target_type', 'weapon_type', 'target1', 'attacker']

for col in cols_to_transform:
    Data_Reading[col] = lb.fit_transform(Data_Reading[col])

country_values = [83, 132, 0, 79, 34, 137, 138, 186, 129, 177]
Data_Reading10 = Data_Reading[Data_Reading['country'].isin(country_values)]

y = Data_Reading10.success

X = Data_Reading10[["year", "month", "day", "extended","country","vicinity","multiple","suicide","attack_type","target_type",
         "target1","attacker" ,"guncertain1","weapon_type","killed","wounded","property","ishostkid", "casualities", "nclass"]]

"""Splitting Dataset"""

Data_Reading_copy_new.info()

# Sample figsize in inches
fig, ax = plt.subplots(figsize=(20,10))

# Select numerical columns and calculate correlation
corr = Data_Reading_copy_new[['Year','Day','Month','Country','City','Success_rate','Attack_type','Target','Killed','Wounded','Weapon_type','Attacker']].corr()

# Create a heatmap of the correlation matrix
sns.heatmap(corr, cmap='YlGnBu', annot_kws={'size':30}, ax=ax, vmax=1, vmin=-1)

# Set the title of the heatmap
ax.set_title("Imbalanced Correlation Matrix", fontsize=18, weight = 'bold')

# Display the figure
plt.show()

Data_Reading2 = Data_Reading_copy_new.select_dtypes(np.number)
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Initialize the DataFrame
vif_data = pd.DataFrame()

# Add the feature names
vif_data["feature"] = Data_Reading2.columns

# Calculate and add the VIF for each feature
vif = []
for i in range(Data_Reading2.shape[1]):
    vif.append(variance_inflation_factor(Data_Reading2.values, i))
vif_data["VIF"] = vif

# Print the DataFrame
print(vif_data)

Data_Reading_copy_new.drop(columns="Year", inplace=True)

Data_Reading3=Data_Reading_copy_new.select_dtypes("object")
Data_Reading3

def replace_non_top_10_values_with_others(x):
    top_10 = x.value_counts().head(10).index
    return x.apply(lambda y: y if y in top_10 else 'others')

Data_Reading3 = Data_Reading3.apply(replace_non_top_10_values_with_others)

from sklearn.preprocessing import OneHotEncoder

# Initialize the OneHotEncoder
encoder = OneHotEncoder(drop='first')

# Fit and transform the data
Data_Reading3_en = encoder.fit_transform(Data_Reading3)

# Convert the result to a DataFrame
Data_Reading3_en = pd.DataFrame(Data_Reading3_en.toarray(), columns=encoder.get_feature_names_out(Data_Reading3.columns))

Data_Reading3_en

Data_Reading2

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
Data_Reading2_sc = pd.DataFrame(sc.fit_transform(Data_Reading2.drop(columns ="Success_rate")),
                         columns=Data_Reading2.drop(columns ="Success_rate").columns,
                         index=Data_Reading2.drop(columns ="Success_rate").index)

Data_Reading_final = pd.concat([Data_Reading2_sc, Data_Reading3_en, Data_Reading2['Success_rate']], axis=1)

Data_Reading_final

!pip install imblearn

from imblearn.over_sampling import SMOTE

# Initialize SMOTE
sm = SMOTE()

# Drop NaN values from the entire dataframe
Data_Reading_final = Data_Reading_final.dropna()

# Define features and target
features = Data_Reading_final.drop(columns='Success_rate')
target = Data_Reading_final['Success_rate']

# Apply SMOTE
X, y = sm.fit_resample(features, target)

plt.figure(figsize=(3,4))
sns.countplot(y)

Data_Reading10

from sklearn.model_selection import train_test_split

# Splitting Data
X = Data_Reading10.drop(columns='success')
y = Data_Reading10['success']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE()

# Define features and target
X_train = Data_Reading10.drop(['success'], axis=1)
y_train = Data_Reading10['success']

# Apply SMOTE to the training data
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Create an object of the classifier
rfc = RandomForestClassifier(random_state=0)

# Train the classifier
rfc.fit(X_train_resampled, y_train_resampled)

# Predict on the test data
preds = rfc.predict(X_test)

print("X_train shape: ", X_train.shape)
print("y_train shape: ", y_train.shape)
print("X_test shape: ", X_test.shape)
print("y_test shape: ", y_test.shape)

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix

def doLogisticRegression(X, y, normalize=False):
  # If normalize option is enabled,
  if normalize:
    # Initialize the StandardScaler
    sc = StandardScaler()

    # Fit and transform the data
    X = sc.fit_transform(X)

  # Instantiate an object from Logistic Regression class.
  lr = LogisticRegression()

  # Perform training and prediction.
  lr.fit(X, y)
  y_pred_lr = lr.predict(X)

  # Return training accuracy and confusion matrix.
  return accuracy_score(y, y_pred_lr), confusion_matrix(y, y_pred_lr), lr

TrainAcc, TrainConf, LR = doLogisticRegression(X_train, y_train, normalize=True)
print(TrainAcc)
print(TrainConf)

from sklearn.metrics import classification_report


# Make predictions on the test data
y_test_pred = LR.predict(X_test)

# Calculate accuracy and confusion matrix
TestAcc = accuracy_score(y_test, y_test_pred)
TestConf = confusion_matrix(y_test, y_test_pred)

# Print accuracy and confusion matrix
print("Test Accuracy: ", TestAcc)
print("Test Confusion Matrix: \n", TestConf)

# Calculate and print classification report
report = classification_report(y_test, y_test_pred)
print("Classification Report: \n", report)

roc_auc_score(y_test, y_test_pred)

!pip install --upgrade scikit-learn

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Assuming y_test are your true labels and y_pred are your predicted labels
y_pred = LR.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

print(classification_report(y_test, y_test_pred))

fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)
plt.plot(fpr,tpr)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
import numpy as np

# Defining the parameter grid
param_distributions = {
    "C": np.logspace(0.5, 1, 10, 100),
    "penalty": ["l1", "l2", "elasticnet"],
    "solver": ["newton-cg", "lbfgs"],
    "max_iter": [100, 500, 1000]
}

# Creating the base model
LR = LogisticRegression()

# Using randomizedsearchcv with cross validation=5
LR_random = RandomizedSearchCV(
    estimator=LR,
    param_distributions=param_distributions,
    cv=5,
    scoring='roc_auc',
    refit=True,
    verbose=3,
    n_iter=100,  # Number of parameter settings that are sampled
    random_state=42  # For reproducibility
)

# Fitting the model
LR_random.fit(X_train, y_train)

LR_random.best_params_

LRTuned=LR_random.best_estimator_.predict(X_test)

roc_auc_score(y_test, LRTuned)

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn_model = knn.fit(X_train,y_train)

pred = knn_model.predict(X_test)

roc_auc_score(y_test,pred)

print(classification_report(y_test,pred))

fpr, tpr, thresholds = roc_curve(y_test, pred)
plt.plot(fpr, tpr)

param_grid = {'weights':['uniform', 'distance'],
              'n_neighbors':[3,5]}

knn = KNeighborsClassifier()
#using gridsearchcv with cross validation=5
knn_grid = GridSearchCV(knn, param_grid, cv = 5, scoring = 'roc_auc', refit = True, verbose = 3)
# Fitting the model
knn_grid.fit(X_train, y_train)

knn_grid.best_params_

knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
knn_best = knn.fit(X_train, y_train)
ypred_knn = knn_best.predict(X_test)

print(classification_report(y_test, pred))

roc_auc_score(y_test, pred)

fpr, tpr, thresholds = roc_curve(y_test, ypred_knn)

plt.plot(fpr, tpr)